{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformers for chemical reactions - reaction prediction\n",
    "\n",
    "\n",
    "![](https://pubs.acs.org/cms/10.1021/acscentsci.9b00576/asset/images/medium/oc9b00576_0009.gif)\n",
    "</br><center>Figure 1: SMILES-to-SMILES translation with the Molecular Transformer</center>\n",
    "\n",
    "## Table of content:\n",
    "#### Setup\n",
    "* [Data download](#first-bullet)\n",
    "* [Load the data](#second-bullet)\n",
    "* [Tokenization](#third-bullet)\n",
    "\n",
    "#### OpenNMT-py main steps\n",
    "* [Building the vocab](#fourth-bullet)\n",
    "* [Training the Molecular Transformer](#fifth-bullet)\n",
    "* [Testing](#sixth-bullet)\n",
    "\n",
    "#### Additional stuff\n",
    "* [Improvements](#seventh-bullet)\n",
    "* [Further steps](#eighth-bullet)\n",
    "* [Publications](#ninth-bullet)\n",
    "\n",
    "We start by installing [OpenNMT-py](https://github.com/OpenNMT/OpenNMT-py), a common Python neural machine translation framework, and [RDKit](https://www.rdkit.org), the open-source python cheminformatics Swiss army knife."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ngSQ6PEECKts",
    "outputId": "060246a0-b370-4ebc-b99c-257570896a35"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "if IN_COLAB:\n",
    "    !pip install rdkit-pypi==2022.3.1\n",
    "    !pip install pip install OpenNMT-py==2.2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZF5nnY_QCRlM"
   },
   "outputs": [],
   "source": [
    "import gdown\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "tqdm.pandas()\n",
    "from rdkit import Chem\n",
    "\n",
    "# to display molecules\n",
    "from rdkit.Chem.Draw import IPythonConsole\n",
    "from rdkit.Chem import Draw\n",
    "IPythonConsole.ipython_useSVG=True\n",
    "\n",
    "\n",
    "# disable RDKit warnings\n",
    "from rdkit import RDLogger\n",
    "RDLogger.DisableLog('rdApp.*') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data download <a class=\"anchor\" id=\"first-bullet\"></a>\n",
    "\n",
    "In this short tutorial, we will look at USPTO_480k, which is a frequently used reaction prediction benchmark dataset. Please note that it does not contain stereochemistry. The original USPTO data can be downloaded from [figshare](https://figshare.com/articles/dataset/Chemical_reactions_from_US_patents_1976-Sep2016_/5104873)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mpXvhvA5C8Zw"
   },
   "outputs": [],
   "source": [
    "def download_data():\n",
    "    # links from https://github.com/coleygroup/Graph2SMILES/blob/main/scripts/download_raw_data.py\n",
    "    USPTO_480k_links= [\n",
    "            (\"https://drive.google.com/uc?id=1RysNBvB2rsMP0Ap9XXi02XiiZkEXCrA8\", \"src-train.txt\"),\n",
    "            (\"https://drive.google.com/uc?id=1CxxcVqtmOmHE2nhmqPFA6bilavzpcIlb\", \"tgt-train.txt\"),\n",
    "            (\"https://drive.google.com/uc?id=1FFN1nz2yB4VwrpWaBuiBDzFzdX3ONBsy\", \"src-val.txt\"),\n",
    "            (\"https://drive.google.com/uc?id=1pYCjWkYvgp1ZQ78EKQBArOvt_2P1KnmI\", \"tgt-val.txt\"),\n",
    "            (\"https://drive.google.com/uc?id=10t6pHj9yR8Tp3kDvG0KMHl7Bt_TUbQ8W\", \"src-test.txt\"),\n",
    "            (\"https://drive.google.com/uc?id=1FeGuiGuz0chVBRgePMu0pGJA4FVReA-b\", \"tgt-test.txt\")\n",
    "        ]\n",
    "    data_path = 'USPTO_480k'\n",
    "    os.makedirs(data_path, exist_ok=True)\n",
    "    for url, name in USPTO_480k_links:\n",
    "        target_path = os.path.join(data_path, name)\n",
    "        if not os.path.exists(target_path):\n",
    "            gdown.download(url, target_path, quiet=False)\n",
    "        else:\n",
    "            print(f\"{target_path} already exists\")\n",
    "\n",
    "def canonicalize_smiles(smiles, verbose=False): # will raise an Exception if invalid SMILES\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if mol is not None:\n",
    "        return Chem.MolToSmiles(mol)\n",
    "    else:\n",
    "        if verbose:\n",
    "            print(f'{smiles} is invalid.')\n",
    "        return ''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uB1gkRYEEjPr",
    "outputId": "d2915598-996e-486d-a240-7b871bdf2d8d"
   },
   "outputs": [],
   "source": [
    "!rm -rf sample_data\n",
    "download_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 358
    },
    "id": "zSi_C8J0LC2z",
    "outputId": "c95a4f76-92aa-43d1-9525-5723c3f7357d"
   },
   "source": [
    "# Load the data <a class=\"anchor\" id=\"second-bullet\"></a>\n",
    "\n",
    "Ideally you would make sure that all SMILES are canonicalized but here we will skip this for time reasons and assume that all SMILES were already canonicalized. The full canonicalization could take ~20 minutes.\n",
    "\n",
    "```python\n",
    "line_count = !cat USPTO_480k/src-train.txt | wc -l\n",
    "total = int(line_count[0])\n",
    "with open('USPTO_480k/src-train.txt', 'r') as f:\n",
    "    precursors_train = [canonicalize_smiles(line.strip().replace(' ', '')) for line in tqdm(f, total=total)]\n",
    "with open('USPTO_480k/tgt-train.txt', 'r') as f:\n",
    "    products_train = [canonicalize_smiles(line.strip().replace(' ', '')) for line in tqdm(f, total=total)]\n",
    "    \n",
    "line_count = !cat USPTO_480k/src-val.txt | wc -l\n",
    "total = int(line_count[0])\n",
    "with open('USPTO_480k/src-val.txt', 'r') as f:\n",
    "    precursors_val = [canonicalize_smiles(line.strip().replace(' ', '')) for line in tqdm(f, total=total)]\n",
    "with open('USPTO_480k/tgt-val.txt', 'r') as f:\n",
    "    products_val = [canonicalize_smiles(line.strip().replace(' ', '')) for line in tqdm(f, total=total)]\n",
    "    \n",
    "line_count = !cat USPTO_480k/src-test.txt | wc -l\n",
    "total = int(line_count[0])\n",
    "with open('USPTO_480k/src-test.txt', 'r') as f:\n",
    "    precursors_test = [canonicalize_smiles(line.strip().replace(' ', '')) for line in tqdm(f, total=total)]\n",
    "with open('USPTO_480k/tgt-test.txt', 'r') as f:\n",
    "    products_test = [canonicalize_smiles(line.strip().replace(' ', '')) for line in tqdm(f, total=total)]\n",
    "```\n",
    "\n",
    "Here we will simply read the data and load it into pandas dataframes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2GfM7drBxtRq"
   },
   "outputs": [],
   "source": [
    "with open('USPTO_480k/src-train.txt', 'r') as f:\n",
    "    precursors_train = [line.strip().replace(' ', '') for line in f]\n",
    "with open('USPTO_480k/tgt-train.txt', 'r') as f:\n",
    "    products_train = [line.strip().replace(' ', '') for line in f]\n",
    "with open('USPTO_480k/src-val.txt', 'r') as f:\n",
    "    precursors_val = [line.strip().replace(' ', '') for line in f]\n",
    "with open('USPTO_480k/tgt-val.txt', 'r') as f:\n",
    "    products_val = [line.strip().replace(' ', '') for line in f]\n",
    "with open('USPTO_480k/src-test.txt', 'r') as f:\n",
    "    precursors_test = [line.strip().replace(' ', '') for line in f]\n",
    "with open('USPTO_480k/tgt-test.txt', 'r') as f:\n",
    "    products_test = [line.strip().replace(' ', '') for line in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F5Y8pCo-L6qI"
   },
   "outputs": [],
   "source": [
    "train_df = pd.DataFrame({'precursors': precursors_train, 'products': products_train})\n",
    "print(f\"The training set contains {train_df.shape[0]} reactions.\")\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ccOYv0XOMIFQ"
   },
   "outputs": [],
   "source": [
    "val_df = pd.DataFrame({'precursors': precursors_val, 'products': products_val})\n",
    "print(f\"The validation set contains {val_df.shape[0]} reactions.\")\n",
    "val_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pv0AxoTEMM0g"
   },
   "outputs": [],
   "source": [
    "test_df = pd.DataFrame({'precursors': precursors_test, 'products': products_test})\n",
    "print(f\"The test set contains {test_df.shape[0]} reactions.\")\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sanity check - canoncalization\n",
    "There is no standard for the canonicalization of SMILES. We might find SMILES that differ... A potential reason for this is that the canonicalization has changed with a newer RDKit version. So, always state the RDKit version that you are working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "line_count = !cat USPTO_480k/src-val.txt | wc -l\n",
    "total = int(line_count[0])+1\n",
    "with open('USPTO_480k/src-val.txt', 'r') as f:\n",
    "    can_precursors_val = [canonicalize_smiles(line.strip().replace(' ', '')) for line in tqdm(f, total=total)]\n",
    "\n",
    "for smiles, can_smiles in zip(precursors_val, can_precursors_val):\n",
    "    try:\n",
    "        assert smiles == can_smiles\n",
    "    except AssertionError:\n",
    "        print(smiles)\n",
    "        print(can_smiles)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization <a class=\"anchor\" id=\"third-bullet\"></a>\n",
    "\n",
    "To be able to train a language model, we need to split the strings into tokens.\n",
    "\n",
    "We take the regex pattern introduced in the [Molecular Transformer](https://pubs.acs.org/doi/abs/10.1021/acscentsci.9b00576)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SMI_REGEX_PATTERN =  r\"(\\%\\([0-9]{3}\\)|\\[[^\\]]+]|Br?|Cl?|N|O|S|P|F|I|b|c|n|o|s|p|\\||\\(|\\)|\\.|=|#|-|\\+|\\\\|\\/|:|~|@|\\?|>>?|\\*|\\$|\\%[0-9]{2}|[0-9])\"\n",
    "\n",
    "def smiles_tokenizer(smiles):\n",
    "    smiles_regex = re.compile(SMI_REGEX_PATTERN)\n",
    "    tokens = [token for token in smiles_regex.findall(smiles)]\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Tokenizing training set')\n",
    "train_df['tokenized_precursors'] = train_df.precursors.progress_apply(lambda smi: smiles_tokenizer(smi))\n",
    "train_df['tokenized_products'] = train_df.products.progress_apply(lambda smi: smiles_tokenizer(smi))\n",
    "print('Tokenizing validation set')\n",
    "val_df['tokenized_precursors'] = val_df.precursors.progress_apply(lambda smi: smiles_tokenizer(smi))\n",
    "val_df['tokenized_products'] = val_df.products.progress_apply(lambda smi: smiles_tokenizer(smi))\n",
    "print('Tokenizing test set')\n",
    "test_df['tokenized_precursors'] = test_df.precursors.progress_apply(lambda smi: smiles_tokenizer(smi))\n",
    "test_df['tokenized_products'] = test_df.products.progress_apply(lambda smi: smiles_tokenizer(smi))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the preprocessed data set\n",
    "\n",
    "Don't forget to shuffle the training set before saving it. At least earlier versions of OpenNMT-py would not shuffle it during preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffled_train_df = train_df.sample(frac=1., random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'USPTO_480k_preprocessed'\n",
    "\n",
    "os.makedirs(data_path, exist_ok=True)\n",
    "with open(os.path.join(data_path, 'precursors-train.txt'), 'w') as f:\n",
    "    f.write('\\n'.join(shuffled_train_df.tokenized_precursors.values))\n",
    "with open(os.path.join(data_path, 'products-train.txt'), 'w') as f:\n",
    "    f.write('\\n'.join(shuffled_train_df.tokenized_products.values))\n",
    "\n",
    "with open(os.path.join(data_path, 'precursors-val.txt'), 'w') as f:\n",
    "    f.write('\\n'.join(val_df.tokenized_precursors.values))\n",
    "with open(os.path.join(data_path, 'products-val.txt'), 'w') as f:\n",
    "    f.write('\\n'.join(val_df.tokenized_products.values))\n",
    "    \n",
    "with open(os.path.join(data_path, 'precursors-test.txt'), 'w') as f:\n",
    "    f.write('\\n'.join(test_df.tokenized_precursors.values))\n",
    "with open(os.path.join(data_path, 'products-test.txt'), 'w') as f:\n",
    "    f.write('\\n'.join(test_df.tokenized_products.values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the vocab <a class=\"anchor\" id=\"fourth-bullet\"></a>\n",
    "\n",
    "The first step for the [OpenNMT-py pipeline](https://opennmt.net/OpenNMT-py/quickstart.html) is to build the vocabulary.\n",
    "\n",
    "![](https://camo.githubusercontent.com/69fb11841ce1abd51a3fd7f3ed4b424857029ce123521cc301eb48a1e22bee2f/687474703a2f2f6f70656e6e6d742e6769746875622e696f2f73696d706c652d6174746e2e706e67)\n",
    "</br><center>Figure 2: In contrast to a neural machine translation model for human language, we will use an atom-wise vocabulary. </center>\n",
    "\n",
    "\n",
    "Please note:\n",
    "- Typical sequence pairs in machine translation are much shorter than the ones you encounter in chemical reaction prediction. Hence, set a `src_seq_length` and `tgt_seq_length` that is much higher than the maximum you would expect to include all reactions.\n",
    "- With `n_sample` set to `-1` we include the whole dataset.\n",
    "\n",
    "The paths to the training and validation datasets are defined in the `run_config.yaml`:\n",
    "\n",
    "```yaml\n",
    "# https://opennmt.net/OpenNMT-py/quickstart.html\n",
    "# Examples in https://github.com/OpenNMT/OpenNMT-py/tree/master/config\n",
    "\n",
    "## Where the samples will be written\n",
    "save_data: example_run\n",
    "## Where the vocab(s) will be written\n",
    "src_vocab: example_run/uspto.vocab.src\n",
    "tgt_vocab: example_run/uspto.vocab.src\n",
    "# Prevent overwriting existing files in the folder\n",
    "overwrite: true\n",
    "share_vocab: true\n",
    "\n",
    "# Corpus opts:\n",
    "data:\n",
    "    corpus-1:\n",
    "        path_src: USPTO_480k_preprocessed/precursors-train.txt\n",
    "        path_tgt: USPTO_480k_preprocessed/products-train.txt\n",
    "    valid:\n",
    "        path_src: USPTO_480k_preprocessed/precursors-val.txt\n",
    "        path_tgt: USPTO_480k_preprocessed/products-val.txt\n",
    "```\n",
    "\n",
    "As the source (precusors) and the target (products) are represented as SMILES and consist of the same tokens, we share the vocabulary between source and target (`share_vocab: true`). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_url = 'https://raw.githubusercontent.com/schwallergroup/dmds_language_models_for_reactions/main/example_run/run_config.yaml'\n",
    "config_folder = 'example_run'\n",
    "config_name = 'run_config.yaml'\n",
    "\n",
    "os.makedirs(config_folder, exist_ok=True)\n",
    "target_path = os.path.join(config_folder, config_name)\n",
    "if not os.path.exists(target_path):\n",
    "    gdown.download(config_url, target_path, quiet=False)\n",
    "else:\n",
    "    print(f\"{target_path} already exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! onmt_build_vocab -config example_run/run_config.yaml \\\n",
    "    -src_seq_length 1000 -tgt_seq_length 1000 \\\n",
    "    -src_vocab_size 1000 -tgt_vocab_size 1000 \\\n",
    "    -n_sample -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the Molecular Transformer <a class=\"anchor\" id=\"fifth-bullet\"></a>\n",
    "\n",
    "If you look at the `run_config.yaml`, you will see that we have defined some of the training parameters (but not yet the hyperparameters of the model.\n",
    "\n",
    "```yaml\n",
    "# Train on a single GPU\n",
    "world_size: 1\n",
    "gpu_ranks: [0]\n",
    "\n",
    "# Where to save the checkpoints\n",
    "save_model: example_run/model\n",
    "save_checkpoint_steps: 5000\n",
    "keep_checkpoint: 3\n",
    "train_steps: 400000\n",
    "valid_steps: 10000\n",
    "report_every: 100\n",
    "\n",
    "tensorboard: true\n",
    "tensorboard_log_dir: log_dir\n",
    "```\n",
    "\n",
    "The Transformer architecture was published in the [Attention is all you need](https://proceedings.neurips.cc/paper/7181-attention-is-all-you-need) paper by Vaswani et al. (NeurIPS, 2017). The model sizes (65 to 212M parameters) in that paper were larger than what we use for reaction prediction (20M parameters). \n",
    "\n",
    "![](https://raw.githubusercontent.com/nlp-with-transformers/notebooks/main/images/chapter01_self-attention.png)\n",
    "</br><center>Figure 3: Transformer model (source: https://github.com/nlp-with-transformers). </center>\n",
    "\n",
    "Illustrated transformer blogposts:\n",
    "- https://nlp.seas.harvard.edu/2018/04/03/attention.html\n",
    "- https://jalammar.github.io/illustrated-transformer/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters from https://github.com/rxn4chemistry/OpenNMT-py/tree/carbohydrate_transformer\n",
    "!onmt_train -config example_run/run_config.yaml \\\n",
    "        -seed 42 -gpu_ranks 0  \\\n",
    "        -param_init 0 \\\n",
    "        -param_init_glorot -max_generator_batches 32 \\\n",
    "        -batch_type tokens -batch_size 6144\\\n",
    "         -normalization tokens -max_grad_norm 0  -accum_count 4 \\\n",
    "        -optim adam -adam_beta1 0.9 -adam_beta2 0.998 -decay_method noam  \\\n",
    "        -warmup_steps 8000 -learning_rate 2 -label_smoothing 0.0 \\\n",
    "        -layers 4 -rnn_size  384 -word_vec_size 384 \\\n",
    "        -encoder_type transformer -decoder_type transformer \\\n",
    "        -dropout 0.1 -position_encoding -share_embeddings  \\\n",
    "        -global_attention general -global_attention_function softmax \\\n",
    "        -self_attn_type scaled-dot -heads 8 -transformer_ff 2048 \\\n",
    "        -tensorboard True -tensorboard_log_dir log_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training can take more than 24 hours on a single GPU. Hence, we will download the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model_url = 'https://drive.google.com/uc?id=1ywJCJHunoPTB5wr6KdZ8aLv7tMFMBHNy'\n",
    "model_folder = 'models'\n",
    "model_name = 'USPTO480k_model_step_400000.pt'\n",
    "\n",
    "os.makedirs(model_folder, exist_ok=True)\n",
    "target_path = os.path.join(model_folder, model_name)\n",
    "if not os.path.exists(target_path):\n",
    "    gdown.download(trained_model_url, target_path, quiet=False)\n",
    "else:\n",
    "    print(f\"{target_path} already exists\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating the model <a class=\"anchor\" id=\"sixth-bullet\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!onmt_translate -model models/USPTO480k_model_step_400000.pt -gpu 0 \\\n",
    "    --src USPTO_480k_preprocessed/precursors-val.txt \\\n",
    "    --tgt USPTO_480k_preprocessed/products-val.txt \\\n",
    "    --output results/USPTO480k_model_step_400000_val_predictions.txt \\\n",
    "    --n_best 5 --beam_size 10 --max_length 300 --batch_size 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/pschwllr/MolecularTransformer/blob/master/score_predictions.py\n",
    "\n",
    "n_best = 5 # top-5 predictions were outputted\n",
    "predictions = [[] for i in range(n_best)]\n",
    "\n",
    "with open('USPTO_480k_preprocessed/products-val.txt', 'r') as f:\n",
    "    targets = [line.strip().replace(' ', '') for line in f]\n",
    "\n",
    "evaluation_df = pd.DataFrame(targets)\n",
    "evaluation_df.columns = ['target']\n",
    "\n",
    "with open('USPTO_480k_preprocessed/precursors-val.txt', 'r') as f:\n",
    "    precursors = [line.strip().replace(' ', '') for line in f]\n",
    "evaluation_df['precursors'] = precursors\n",
    "\n",
    "total = len(evaluation_df)\n",
    "\n",
    "with open('results/USPTO480k_model_step_400000_val_predictions_precomputed.txt', 'r') as f:\n",
    "    \n",
    "    for i, line in enumerate(f):\n",
    "        predictions[i % n_best].append(''.join(line.strip().split(' ')))\n",
    "for i, preds in enumerate(predictions):\n",
    "    evaluation_df['prediction_{}'.format(i + 1)] = preds\n",
    "    evaluation_df['canonical_prediction_{}'.format(i + 1)] = evaluation_df['prediction_{}'.format(i + 1)].progress_apply(\n",
    "        lambda x: canonicalize_smiles(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rank(row, col_name, max_rank):\n",
    "    for i in range(1, max_rank+1):\n",
    "        if row['target'] == row['{}{}'.format(col_name, i)]:\n",
    "            return i\n",
    "    return 0\n",
    "evaluation_df['rank'] = evaluation_df.progress_apply(lambda row: get_rank(row, 'canonical_prediction_', n_best), axis=1)\n",
    "\n",
    "correct = 0\n",
    "\n",
    "for i in range(1, n_best+1):\n",
    "    correct += (evaluation_df['rank'] == i).sum()\n",
    "    invalid_smiles = (test_df['canonical_prediction_{}'.format(i)] == '').sum()\n",
    "    \n",
    "    print('Top-{}: {:.1f}% || Invalid SMILES {:.2f}%'.format(i, correct/total*100,\n",
    "                                                                 invalid_smiles/total*100))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's draw some of the reactions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit import Chem\n",
    "from rdkit.Chem import Draw\n",
    "from rdkit.Chem import rdChemReactions\n",
    "from rdkit.Chem.Draw import rdMolDraw2D\n",
    "from rdkit.Chem.Draw import IPythonConsole\n",
    "from IPython.display import SVG, display\n",
    "\n",
    "\n",
    "# https://gist.github.com/greglandrum/61c1e751b453c623838759609dc41ef1\n",
    "def draw_chemical_reaction(smiles,useSmiles=True,  highlightByReactant=False, notesAtomMaps=True, font_scale=1.5):\n",
    "    rxn = rdChemReactions.ReactionFromSmarts(smiles,useSmiles=useSmiles)\n",
    "    trxn = rdChemReactions.ChemicalReaction(rxn)\n",
    "    # move atom maps to be annotations:\n",
    "    if notesAtomMaps:\n",
    "        for m in trxn.GetReactants():\n",
    "            moveAtomMapsToNotes(m)\n",
    "        for m in trxn.GetProducts():\n",
    "            moveAtomMapsToNotes(m)\n",
    "    d2d = rdMolDraw2D.MolDraw2DSVG(800,300)\n",
    "    d2d.drawOptions().annotationFontScale=font_scale\n",
    "    d2d.DrawReaction(trxn,highlightByReactant=highlightByReactant)\n",
    "    d2d.FinishDrawing()\n",
    "\n",
    "    return d2d.GetDrawingText()\n",
    "\n",
    "def moveAtomMapsToNotes(m):\n",
    "    for at in m.GetAtoms():\n",
    "        if at.GetAtomMapNum():\n",
    "            at.SetProp(\"atomNote\",str(at.GetAtomMapNum()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i, row in evaluation_df[evaluation_df['rank']==1].sample(5, random_state=1).iterrows():\n",
    "    rxn_smiles = f\"{row['precursors']}>>{row['canonical_prediction_1']}\"\n",
    "    display(SVG(draw_chemical_reaction(rxn_smiles)))\n",
    "    print(rxn_smiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, row in evaluation_df[evaluation_df['rank']==0].sample(5, random_state=1).iterrows():\n",
    "    rxn_smiles = f\"{row['precursors']}>>{row['target']}.{row['canonical_prediction_1']}\"\n",
    "    display(SVG(draw_chemical_reaction(rxn_smiles)))\n",
    "    print(rxn_smiles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improvements to the chemical reaction language models <a class=\"anchor\" id=\"seventh-bullet\"></a>\n",
    "\n",
    "One of the improvements compared to the plain Molecular Transformer model, that was done in the past is data augmentation. \n",
    "- [Molecular Transformer](https://pubs.acs.org/doi/abs/10.1021/acscentsci.9b00576) -> one non-canonical copy of each precursors\n",
    "- [Augmented Molecular Transformer](https://www.nature.com/articles/s41467-020-19266-y) -> extensive data augmentation on precursors and products sides\n",
    "\n",
    "If you have a small dataset of more challenging reactions you could use transfer learning, as we explored in [Transfer learning enables the molecular transformer to predict regio- and stereoselective reactions on carbohydrates](https://www.nature.com/articles/s41467-020-18671-7)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data augmentations for reaction SMILES\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MGrUMhzzMRTR"
   },
   "outputs": [],
   "source": [
    "# What if now we wanted to do some data augmentation on the training set\n",
    "\n",
    "def randomize_smiles(smiles, random_type=\"rotated\"):\n",
    "    \"\"\"\n",
    "    # https://github.com/rxn4chemistry/rxn_yields/blob/master/nbs/06_data_augmentation.ipynb\n",
    "    Inspired from: https://github.com/undeadpixel/reinvent-randomized and https://github.com/GLambard/SMILES-X\n",
    "    Returns a random SMILES given a SMILES of a molecule.\n",
    "    :param mol: A Mol object\n",
    "    :param random_type: The type (unrestricted, restricted, rotated) of randomization performed.\n",
    "    :return : A random SMILES string of the same molecule or None if the molecule is invalid.\n",
    "    \"\"\"\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if not mol:\n",
    "        print(f\"{smiles} not valid.\")\n",
    "        return None\n",
    "\n",
    "    if random_type == \"unrestricted\":\n",
    "        return Chem.MolToSmiles(mol, canonical=False, doRandom=True, isomericSmiles=True)\n",
    "    elif random_type == \"restricted\":\n",
    "        new_atom_order = list(range(mol.GetNumAtoms()))\n",
    "        random.shuffle(new_atom_order)\n",
    "        random_mol = Chem.RenumberAtoms(mol, newOrder=new_atom_order)\n",
    "        return Chem.MolToSmiles(random_mol, canonical=False, isomericSmiles=True)\n",
    "    elif random_type == 'rotated':\n",
    "        n_atoms = mol.GetNumAtoms()\n",
    "        rotation_index = random.randint(0, n_atoms-1)\n",
    "        atoms = list(range(n_atoms))\n",
    "        new_atoms_order = (atoms[rotation_index%len(atoms):]+atoms[:rotation_index%len(atoms)])\n",
    "        rotated_mol = Chem.RenumberAtoms(mol,new_atoms_order)\n",
    "        return Chem.MolToSmiles(rotated_mol, canonical=False, isomericSmiles=True)\n",
    "    raise ValueError(\"Type '{}' is not valid\".format(random_type))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TI-zZVWgNcHc"
   },
   "outputs": [],
   "source": [
    "example_smi = 'O=C1C2=C(N=CN2C)N(C(=O)N1C)C'\n",
    "mol = Chem.MolFromSmiles(example_smi)\n",
    "print(f\"The canonical SMILES of this caffeine molecule is: {Chem.MolToSmiles(mol)}\")\n",
    "mol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FnX839OUNcr7"
   },
   "outputs": [],
   "source": [
    "# different starting atom\n",
    "rotated_random_smiles = []\n",
    "for i in range (500):\n",
    "    rotated_random_smiles.append(randomize_smiles(example_smi))\n",
    "print(len(set(rotated_random_smiles)))\n",
    "set(rotated_random_smiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MM61fdm5PKX5"
   },
   "outputs": [],
   "source": [
    "restricted_random_smiles = []\n",
    "for i in range (500):\n",
    "    restricted_random_smiles.append(randomize_smiles(example_smi, 'restricted'))\n",
    "print(len(set(restricted_random_smiles)))\n",
    "list(set(restricted_random_smiles))[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SSJiNnttPZLO"
   },
   "outputs": [],
   "source": [
    "unrestricted_random_smiles = []\n",
    "for i in range (10000):\n",
    "    unrestricted_random_smiles.append(randomize_smiles(example_smi, random_type='unrestricted'))\n",
    "print(len(set(unrestricted_random_smiles)))\n",
    "list(set(unrestricted_random_smiles))[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "842reYtbPoQ9"
   },
   "outputs": [],
   "source": [
    "recanonicalised_smiles = set([Chem.MolToSmiles(Chem.MolFromSmiles(smiles)) for smiles in unrestricted_random_smiles])\n",
    "assert len(recanonicalised_smiles) == 1\n",
    "recanonicalised_smiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xaC-1e4RzSVC",
    "outputId": "c749fe1f-e5ca-41fa-e500-37e9f13192e9"
   },
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    print(randomize_smiles(can_smiles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p2jfdr_UQDkH"
   },
   "outputs": [],
   "source": [
    "# we will include a rotated copy of all the training reactions\n",
    "\n",
    "rotated_train_precursors = [randomize_smiles(precursors) for precursors in tqdm(train_df.precursors)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8pXEayqQqyJt"
   },
   "outputs": [],
   "source": [
    "rotated_train_df = pd.DataFrame({'precursors': rotated_train_precursors, 'products': products_train})\n",
    "total_train_df = pd.concat([train_df, rotated_train_df])\n",
    "total_train_df.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Tokenizing training set')\n",
    "total_train_df['tokenized_precursors'] = total_train_df.precursors.apply(lambda smi: smiles_tokenizer(smi))\n",
    "total_train_df['tokenized_products'] = total_train_df.products.apply(lambda smi: smiles_tokenizer(smi))\n",
    "print('Tokenizing validation set')\n",
    "val_df['tokenized_precursors'] = val_df.precursors.apply(lambda smi: smiles_tokenizer(smi))\n",
    "val_df['tokenized_products'] = val_df.products.apply(lambda smi: smiles_tokenizer(smi))\n",
    "print('Tokenizing test set')\n",
    "test_df['tokenized_precursors'] = test_df.precursors.apply(lambda smi: smiles_tokenizer(smi))\n",
    "test_df['tokenized_products'] = test_df.products.apply(lambda smi: smiles_tokenizer(smi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remember to shuffle your training data :)\n",
    "\n",
    "shuffled_total_train_df = total_train_df.sample(frac=1., random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffled_total_train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IFuhH4TsxjRE"
   },
   "outputs": [],
   "source": [
    "data_path = 'USPTO_480k_augm_preprocessed'\n",
    "\n",
    "os.makedirs(data_path, exist_ok=True)\n",
    "with open(os.path.join(data_path, 'precursors-train.txt'), 'w') as f:\n",
    "    f.write('\\n'.join(shuffled_total_train_df.tokenized_precursors.values))\n",
    "with open(os.path.join(data_path, 'products-train.txt'), 'w') as f:\n",
    "    f.write('\\n'.join(shuffled_total_train_df.tokenized_products.values))\n",
    "\n",
    "with open(os.path.join(data_path, 'precursors-val.txt'), 'w') as f:\n",
    "    f.write('\\n'.join(val_df.tokenized_precursors.values))\n",
    "with open(os.path.join(data_path, 'products-val.txt'), 'w') as f:\n",
    "    f.write('\\n'.join(val_df.tokenized_products.values))\n",
    "    \n",
    "with open(os.path.join(data_path, 'precursors-test.txt'), 'w') as f:\n",
    "    f.write('\\n'.join(test_df.tokenized_precursors.values))\n",
    "with open(os.path.join(data_path, 'products-test.txt'), 'w') as f:\n",
    "    f.write('\\n'.join(test_df.tokenized_products.values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build vocab, train, and test\n",
    "\n",
    "Start by writing a `example_run/run_config_augm.yaml` file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xsTFqoLZsTSF"
   },
   "outputs": [],
   "source": [
    "! onmt_build_vocab -config example_run/run_config_augm.yaml ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! onmt_train ... "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EaTbQWErstvf"
   },
   "source": [
    "# Further steps <a class=\"anchor\" id=\"eighth-bullet\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RXN for Chemistry\n",
    "You can access all the trained models from [RXN for Chemistry](https://rxn.res.ibm.com) through the rxn4chemistry Python API:\n",
    "https://github.com/rxn4chemistry/rxn4chemistry\n",
    "\n",
    "There are examples in:\n",
    "https://github.com/rxn4chemistry/rxn4chemistry/tree/master/examples\n",
    "\n",
    "\n",
    "\n",
    "## RXNFP and DRFP -> chemical reaction fingerprints\n",
    "- Data driven reaction fingerprint: https://github.com/rxn4chemistry/rxnfp with tutorial on https://rxn4chemistry.github.io/rxnfp/\n",
    "- Engineered reaction fingerprint: https://github.com/reymond-group/drfp with great examples in https://github.com/reymond-group/drfp/tree/main/notebooks\n",
    "\n",
    "## Atom-mapping \n",
    "When Transformers are trained on large datasets of unlabelled reactions represented as SMILES, they learn how atom rearrange during chemical reactions. We used this signal to build [RXNMapper](http://rxnmapper.ai). The code can be found in: https://github.com/rxn4chemistry/rxnmapper\n",
    "\n",
    "If you just want to play with the demo:\n",
    "http://rxnmapper.ai/demo.html?rxn=CC(C)S.CN(C)C%253DO.Fc1cccnc1F.O%253DC(%255BO-%255D)%255BO-%255D.%255BK%252B%255D.%255BK%252B%255D%253E%253ECC(C)Sc1ncccc1F&selectedLayer=10&selectedHead=5&selectedTokenSide=null&selectedTokenInd=null\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Publications <a class=\"anchor\" id=\"ninth-bullet\"></a>\n",
    "### Reaction prediction\n",
    "- [“Found in Translation”: predicting outcomes of complex organic chemistry reactions using neural sequence-to-sequence models](https://pubs.rsc.org/en/content/articlehtml/2018/sc/c8sc02339e) \n",
    "- [Molecular Transformer: A Model for Uncertainty-Calibrated Chemical Reaction Prediction](https://pubs.acs.org/doi/abs/10.1021/acscentsci.9b00576)\n",
    "- [Transfer learning enables the molecular transformer to predict regio- and stereoselective reactions on carbohydrates](https://www.nature.com/articles/s41467-020-18671-7)\n",
    "\n",
    "### Retrosynthesis\n",
    "- [Predicting retrosynthetic pathways using transformer-based models and a hyper-graph exploration strategy](https://pubs.rsc.org/en/content/articlehtml/2020/sc/c9sc05704h)\n",
    "\n",
    "### Reaction fingerprints\n",
    "- [Mapping the space of chemical reactions using attention-based neural networks](http://rdcu.be/cenmd)\n",
    "- [Reaction classification and yield prediction using the differential reaction fingerprint DRFP](https://pubs.rsc.org/en/content/articlehtml/2022/dd/d1dd00006c)\n",
    "\n",
    "### Yield prediction\n",
    "- [Prediction of chemical reaction yields using deep learning](https://iopscience.iop.org/article/10.1088/2632-2153/abc81d/meta)\n",
    "- [Data augmentation strategies to improve reaction yield predictions and estimate uncertainty](https://chemrxiv.org/engage/chemrxiv/article-details/60c75258702a9b726c18c101)\n",
    "\n",
    "### Atom-mapping\n",
    "- [Extraction of organic chemistry grammar from unsupervised learning of chemical reactions](https://www.science.org/doi/10.1126/sciadv.abe4166)\n",
    "\n",
    "### Extensive review\n",
    "- [Machine Intelligence for Chemical Reaction Space](https://wires.onlinelibrary.wiley.com/doi/full/10.1002/wcms.1604)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "test_reaction_prediction.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

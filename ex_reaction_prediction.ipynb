{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "test_reaction_prediction.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install rdkit-pypi==2022.3.1\n",
        "!pip install pip install OpenNMT-py==2.2.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ngSQ6PEECKts",
        "outputId": "060246a0-b370-4ebc-b99c-257570896a35"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rdkit-pypi==2022.3.1\n",
            "  Downloading rdkit_pypi-2022.3.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (22.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 22.5 MB 1.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.19 in /usr/local/lib/python3.7/dist-packages (from rdkit-pypi==2022.3.1) (1.21.5)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from rdkit-pypi==2022.3.1) (7.1.2)\n",
            "Installing collected packages: rdkit-pypi\n",
            "Successfully installed rdkit-pypi-2022.3.1\n",
            "Requirement already satisfied: pip in /usr/local/lib/python3.7/dist-packages (21.1.3)\n",
            "Collecting install\n",
            "  Downloading install-1.3.5-py3-none-any.whl (3.2 kB)\n",
            "Collecting OpenNMT-py==2.2.0\n",
            "  Downloading OpenNMT_py-2.2.0-py3-none-any.whl (216 kB)\n",
            "\u001b[K     |████████████████████████████████| 216 kB 3.6 MB/s \n",
            "\u001b[?25hCollecting configargparse\n",
            "  Downloading ConfigArgParse-1.5.3-py3-none-any.whl (20 kB)\n",
            "Collecting torchtext==0.5.0\n",
            "  Downloading torchtext-0.5.0-py3-none-any.whl (73 kB)\n",
            "\u001b[K     |████████████████████████████████| 73 kB 901 kB/s \n",
            "\u001b[?25hCollecting waitress\n",
            "  Downloading waitress-2.1.1-py3-none-any.whl (57 kB)\n",
            "\u001b[K     |████████████████████████████████| 57 kB 4.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from OpenNMT-py==2.2.0) (3.13)\n",
            "Requirement already satisfied: tensorboard>=2.3 in /usr/local/lib/python3.7/dist-packages (from OpenNMT-py==2.2.0) (2.8.0)\n",
            "Collecting pyonmttok<2,>=1.23\n",
            "  Downloading pyonmttok-1.31.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (16.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 16.6 MB 56 kB/s \n",
            "\u001b[?25hRequirement already satisfied: flask in /usr/local/lib/python3.7/dist-packages (from OpenNMT-py==2.2.0) (1.1.4)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from OpenNMT-py==2.2.0) (1.10.0+cu111)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchtext==0.5.0->OpenNMT-py==2.2.0) (1.21.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from torchtext==0.5.0->OpenNMT-py==2.2.0) (1.15.0)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 5.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchtext==0.5.0->OpenNMT-py==2.2.0) (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torchtext==0.5.0->OpenNMT-py==2.2.0) (4.64.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.3->OpenNMT-py==2.2.0) (57.4.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.3->OpenNMT-py==2.2.0) (0.37.1)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.3->OpenNMT-py==2.2.0) (1.0.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.3->OpenNMT-py==2.2.0) (1.8.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.3->OpenNMT-py==2.2.0) (1.35.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.3->OpenNMT-py==2.2.0) (0.4.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.3->OpenNMT-py==2.2.0) (0.6.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.3->OpenNMT-py==2.2.0) (1.0.1)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.3->OpenNMT-py==2.2.0) (1.44.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.3->OpenNMT-py==2.2.0) (3.3.6)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.3->OpenNMT-py==2.2.0) (3.17.3)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.3->OpenNMT-py==2.2.0) (4.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.3->OpenNMT-py==2.2.0) (4.2.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.3->OpenNMT-py==2.2.0) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.3->OpenNMT-py==2.2.0) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard>=2.3->OpenNMT-py==2.2.0) (4.11.3)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard>=2.3->OpenNMT-py==2.2.0) (4.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard>=2.3->OpenNMT-py==2.2.0) (3.8.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.3->OpenNMT-py==2.2.0) (0.4.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.5.0->OpenNMT-py==2.2.0) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.5.0->OpenNMT-py==2.2.0) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.5.0->OpenNMT-py==2.2.0) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.5.0->OpenNMT-py==2.2.0) (3.0.4)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.3->OpenNMT-py==2.2.0) (3.2.0)\n",
            "Requirement already satisfied: itsdangerous<2.0,>=0.24 in /usr/local/lib/python3.7/dist-packages (from flask->OpenNMT-py==2.2.0) (1.1.0)\n",
            "Requirement already satisfied: click<8.0,>=5.1 in /usr/local/lib/python3.7/dist-packages (from flask->OpenNMT-py==2.2.0) (7.1.2)\n",
            "Requirement already satisfied: Jinja2<3.0,>=2.10.1 in /usr/local/lib/python3.7/dist-packages (from flask->OpenNMT-py==2.2.0) (2.11.3)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2<3.0,>=2.10.1->flask->OpenNMT-py==2.2.0) (2.0.1)\n",
            "Installing collected packages: sentencepiece, waitress, torchtext, pyonmttok, configargparse, OpenNMT-py, install\n",
            "  Attempting uninstall: torchtext\n",
            "    Found existing installation: torchtext 0.11.0\n",
            "    Uninstalling torchtext-0.11.0:\n",
            "      Successfully uninstalled torchtext-0.11.0\n",
            "Successfully installed OpenNMT-py-2.2.0 configargparse-1.5.3 install-1.3.5 pyonmttok-1.31.0 sentencepiece-0.1.96 torchtext-0.5.0 waitress-2.1.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gdown\n",
        "import os\n",
        "import random\n",
        "import re\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "from tqdm import tqdm\n",
        "from rdkit import Chem\n",
        "\n",
        "# to display molecules\n",
        "from rdkit.Chem.Draw import IPythonConsole\n",
        "from rdkit.Chem import Draw\n",
        "IPythonConsole.ipython_useSVG=True"
      ],
      "metadata": {
        "id": "ZF5nnY_QCRlM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def download_data():\n",
        "  # links from https://github.com/coleygroup/Graph2SMILES/blob/main/scripts/download_raw_data.py\n",
        "  USPTO_480k_links= [\n",
        "            (\"https://drive.google.com/uc?id=1RysNBvB2rsMP0Ap9XXi02XiiZkEXCrA8\", \"src-train.txt\"),\n",
        "            (\"https://drive.google.com/uc?id=1CxxcVqtmOmHE2nhmqPFA6bilavzpcIlb\", \"tgt-train.txt\"),\n",
        "            (\"https://drive.google.com/uc?id=1FFN1nz2yB4VwrpWaBuiBDzFzdX3ONBsy\", \"src-val.txt\"),\n",
        "            (\"https://drive.google.com/uc?id=1pYCjWkYvgp1ZQ78EKQBArOvt_2P1KnmI\", \"tgt-val.txt\"),\n",
        "            (\"https://drive.google.com/uc?id=10t6pHj9yR8Tp3kDvG0KMHl7Bt_TUbQ8W\", \"src-test.txt\"),\n",
        "            (\"https://drive.google.com/uc?id=1FeGuiGuz0chVBRgePMu0pGJA4FVReA-b\", \"tgt-test.txt\")\n",
        "        ]\n",
        "  data_path = 'USPTO_480k'\n",
        "  os.makedirs(data_path, exist_ok=True)\n",
        "  for url, name in USPTO_480k_links:\n",
        "    target_path = os.path.join(data_path, name)\n",
        "    if not os.path.exists(target_path):\n",
        "      gdown.download(url, target_path, quiet=False)\n",
        "    else:\n",
        "      print(f\"{target_path} already exists\")\n",
        "\n",
        "def canonicalize_smiles(smiles): # will raise an Exception if invalid SMILES\n",
        "  return Chem.MolToSmiles(Chem.MolFromSmiles(smiles))\n"
      ],
      "metadata": {
        "id": "mpXvhvA5C8Zw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf sample_data\n",
        "download_data()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uB1gkRYEEjPr",
        "outputId": "d2915598-996e-486d-a240-7b871bdf2d8d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "USPTO_480k/src-train.txt already exists\n",
            "USPTO_480k/tgt-train.txt already exists\n",
            "USPTO_480k/src-val.txt already exists\n",
            "USPTO_480k/tgt-val.txt already exists\n",
            "USPTO_480k/src-test.txt already exists\n",
            "USPTO_480k/tgt-test.txt already exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!head USPTO_480k/src-train.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F8ys2ZrxEkv-",
        "outputId": "93f1e181-427a-4d67-d055-6cff9161ecd1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "C 1 C C O C 1 . C C ( C ) C [Mg+] . C O N ( C ) C ( = O ) c 1 c c c ( O ) n c 1 . [Cl-]\n",
            "C N . O . O = C ( O ) c 1 c c c ( Cl ) c ( [N+] ( = O ) [O-] ) c 1\n",
            "C C n 1 c c ( C ( = O ) O ) c ( = O ) c 2 c c ( F ) c ( - c 3 c c c ( N ) c c 3 ) c c 2 1 . O = C O\n",
            "C C ( C ) = C ( Cl ) N ( C ) C . C O C C ( C ) O c 1 c c ( O c 2 c n c ( C ( = O ) N 3 C C C 3 ) c n 2 ) c c ( C ( = O ) O ) c 1 . C c 1 c n c ( N ) c n 1 . Cl C Cl . c 1 c c n c c 1\n",
            "Cl c 1 c c 2 c ( Cl ) n c ( - c 3 c c n c c 3 ) n c 2 s 1 . N C c 1 c c c ( Cl ) c ( Cl ) c 1\n",
            "C C ( = O ) O . C c 1 c ( Cl ) n n c ( C ( C # N ) c 2 c c c ( F ) c ( C # N ) c 2 ) c 1 C . Cl . O\n",
            "C C ( N ) c 1 c c c ( F ) c ( Cl ) c 1 . N C 1 C C c 2 c c ( F ) c c c 2 1 . O = C ( N 1 C C c 2 c c c ( Cl ) c ( O S ( = O ) ( = O ) C ( F ) ( F ) F ) c 2 C C 1 ) C ( F ) ( F ) F\n",
            "C C ( C ) N 1 C C N ( C ( = O ) c 2 c c c 3 [nH] c ( C ( = O ) N 4 C C N ( S ( C ) ( = O ) = O ) C C 4 ) c c 3 c 2 ) C C 1 . C C O C ( = O ) N 1 C C N C C 1\n",
            "C C ( C ( = O ) O ) C ( = O ) N C c 1 c c c ( F ) c c 1 . C N 1 C ( = O ) C ( N ) c 2 c c c c c 2 - c 2 c c c c c 2 1\n",
            "C C ( = O ) N 1 C C N ( c 2 c c c ( N ) n c 2 ) C C 1 . C C N ( C ( C ) C ) C ( C ) C . C N ( C ) C ( O n 1 n n c 2 c c c n c 2 1 ) = [N+] ( C ) C . C N ( C ) C = O . F [P-] ( F ) ( F ) ( F ) ( F ) F . O = C ( O ) C c 1 c c c ( Br ) c ( C ( F ) ( F ) F ) c 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!head USPTO_480k/tgt-train.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qbzew8y1ErZT",
        "outputId": "52c2ffb5-6ea5-46c5-f35b-22f3525e30e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "C C ( C ) C C ( = O ) c 1 c c c ( O ) n c 1\n",
            "C N c 1 c c c ( C ( = O ) O ) c c 1 [N+] ( = O ) [O-]\n",
            "C C n 1 c c ( C ( = O ) O ) c ( = O ) c 2 c c ( F ) c ( - c 3 c c c ( N C = O ) c c 3 ) c c 2 1\n",
            "C O C C ( C ) O c 1 c c ( O c 2 c n c ( C ( = O ) N 3 C C C 3 ) c n 2 ) c c ( C ( = O ) N c 2 c n c ( C ) c n 2 ) c 1\n",
            "Cl c 1 c c 2 c ( N C c 3 c c c ( Cl ) c ( Cl ) c 3 ) n c ( - c 3 c c n c c 3 ) n c 2 s 1\n",
            "C c 1 c ( Cl ) n n c ( C c 2 c c c ( F ) c ( C # N ) c 2 ) c 1 C\n",
            "C C ( N c 1 c ( Cl ) c c c 2 c 1 C C N ( C ( = O ) C ( F ) ( F ) F ) C C 2 ) c 1 c c c ( F ) c ( Cl ) c 1\n",
            "C C O C ( = O ) N 1 C C N ( C ( = O ) c 2 c c 3 c c ( C ( = O ) N 4 C C N ( C ( C ) C ) C C 4 ) c c c 3 [nH] 2 ) C C 1\n",
            "C C ( C ( = O ) N C c 1 c c c ( F ) c c 1 ) C ( = O ) N C 1 C ( = O ) N ( C ) c 2 c c c c c 2 - c 2 c c c c c 2 1\n",
            "C C ( = O ) N 1 C C N ( c 2 c c c ( N C ( = O ) C c 3 c c c ( Br ) c ( C ( F ) ( F ) F ) c 3 ) n c 2 ) C C 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load data into dataframes"
      ],
      "metadata": {
        "id": "5YscB9xzFAuS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "line_count = !cat USPTO_480k/src-train.txt | wc -l\n",
        "total = int(line_count[0])"
      ],
      "metadata": {
        "id": "GBr6fdJHPvHT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s5dYPxfXPxyS",
        "outputId": "a0cb6476-4fdf-4107-8b8a-188fe1640c97"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "409034"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ideally you would make sure that all SMILES are canonicalized but here we will skip this for time reasons and assume that all SMILES were already canonicalized\n",
        "```\n",
        "line_count = !cat USPTO_480k/src-train.txt | wc -l\n",
        "total = int(line_count[0])\n",
        "with open('USPTO_480k/src-train.txt', 'r') as f:\n",
        "  precursors_train = [canonicalize_smiles(line.strip().replace(' ', '')) for line in tqdm(f, total=total)]\n",
        "with open('USPTO_480k/tgt-train.txt', 'r') as f:\n",
        "  products_train = [canonicalize_smiles(line.strip().replace(' ', '')) for line in tqdm(f, total=total)]\n",
        "line_count = !cat USPTO_480k/src-val.txt | wc -l\n",
        "total = int(line_count[0])\n",
        "with open('USPTO_480k/src-val.txt', 'r') as f:\n",
        "  precursors_val = [canonicalize_smiles(line.strip().replace(' ', '')) for line in tqdm(f, total=total)]\n",
        "with open('USPTO_480k/tgt-val.txt', 'r') as f:\n",
        "  products_val = [canonicalize_smiles(line.strip().replace(' ', '')) for line in tqdm(f, total=total)]\n",
        "line_count = !cat USPTO_480k/src-test.txt | wc -l\n",
        "total = int(line_count[0])\n",
        "with open('USPTO_480k/src-test.txt', 'r') as f:\n",
        "  precursors_test = [canonicalize_smiles(line.strip().replace(' ', '')) for line in tqdm(f, total=total)]\n",
        "with open('USPTO_480k/tgt-test.txt', 'r') as f:\n",
        "  products_test = [canonicalize_smiles(line.strip().replace(' ', '')) for line in tqdm(f, total=total)]\n",
        "```"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 358
        },
        "id": "zSi_C8J0LC2z",
        "outputId": "c95a4f76-92aa-43d1-9525-5723c3f7357d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  2%|▏         | 9710/409034 [00:05<07:35, 877.42it/s]"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-167f33617702>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtotal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline_count\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'USPTO_480k/src-train.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m   \u001b[0mprecursors_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcanonicalize_smiles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'USPTO_480k/tgt-train.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0mproducts_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcanonicalize_smiles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-167f33617702>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtotal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline_count\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'USPTO_480k/src-train.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m   \u001b[0mprecursors_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcanonicalize_smiles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'USPTO_480k/tgt-train.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0mproducts_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcanonicalize_smiles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-da24839b66c1>\u001b[0m in \u001b[0;36mcanonicalize_smiles\u001b[0;34m(smiles)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcanonicalize_smiles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msmiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mChem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMolToSmiles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mChem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMolFromSmiles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msmiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('USPTO_480k/src-train.txt', 'r') as f:\n",
        "  precursors_train = [line.strip().replace(' ', '') for line in f]\n",
        "with open('USPTO_480k/tgt-train.txt', 'r') as f:\n",
        "  products_train = [line.strip().replace(' ', '') for line in f]\n",
        "with open('USPTO_480k/src-val.txt', 'r') as f:\n",
        "  precursors_val = [line.strip().replace(' ', '') for line in f]\n",
        "with open('USPTO_480k/tgt-val.txt', 'r') as f:\n",
        "  products_val = [line.strip().replace(' ', '') for line in f]\n",
        "with open('USPTO_480k/src-test.txt', 'r') as f:\n",
        "  precursors_test = [line.strip().replace(' ', '') for line in f]\n",
        "with open('USPTO_480k/tgt-test.txt', 'r') as f:\n",
        "  products_test = [line.strip().replace(' ', '') for line in f]"
      ],
      "metadata": {
        "id": "2GfM7drBxtRq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "line_count = !cat USPTO_480k/src-val.txt | wc -l\n",
        "total = int(line_count[0])\n",
        "with open('USPTO_480k/src-val.txt', 'r') as f:\n",
        "  can_precursors_val = [canonicalize_smiles(line.strip().replace(' ', '')) for line in tqdm(f, total=total)]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YX_7QTXExLgw",
        "outputId": "c7dbabdc-1d84-4a23-e9c3-b1ddd0336615"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "30000it [00:12, 2445.51it/s]                           \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# we would indeed have another canonicalisation \n",
        "# there is no standard canonicalisation / \n",
        "for smiles, can_smiles in zip(precursors_val, can_precursors_val):\n",
        "  try:\n",
        "    assert smiles == can_smiles\n",
        "  except AssertionError:\n",
        "    print(smiles)\n",
        "    print(can_smiles)\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ZoG9gIDxH9F",
        "outputId": "f6000194-9989-4dd8-c9dc-83a24520bb06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "C1COCCO1.COc1cc2sc3ccc(Br)cc3n3cc(Cc4cccnc4)c(=O)c(c1)c23.N#CC1=C(C#N)C(=O)C(Cl)=C(Cl)C1=O\n",
            "C1COCCO1.COc1cc2c3c(c1)c(=O)c(Cc1cccnc1)cn3-c1cc(Br)ccc1S2.N#CC1=C(C#N)C(=O)C(Cl)=C(Cl)C1=O\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "So_M-wLxzBjk",
        "outputId": "4c7c4397-ce85-4039-e9c8-d35a7c8f0104"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'O=C1C(C#N)=C(C#N)C(=O)C(Cl)=C1Cl.C1COCCO1.COc1cc2c3c(c1)c(=O)c(Cc1cccnc1)cn3-c1cc(Br)ccc1S2'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "LAbMM9tRy2Km"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = pd.DataFrame({'precursors': precursors_train, 'products': products_train})\n",
        "print(f\"The training set contains {train_df.shape[0]} reactions.\")\n",
        "train_df.head()"
      ],
      "metadata": {
        "id": "F5Y8pCo-L6qI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_df = pd.DataFrame({'precursors': precursors_val, 'products': products_val})\n",
        "print(f\"The validation set contains {val_df.shape[0]} reactions.\")\n",
        "val_df.head()"
      ],
      "metadata": {
        "id": "ccOYv0XOMIFQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_df = pd.DataFrame({'precursors': precursors_test, 'products': products_test})\n",
        "print(f\"The test set contains {test_df.shape[0]} reactions.\")\n",
        "test_df.head()"
      ],
      "metadata": {
        "id": "Pv0AxoTEMM0g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# What if now we wanted to do some data augmentation on the training set\n",
        "\n",
        "def randomize_smiles(smiles, random_type=\"rotated\"):\n",
        "    \"\"\"\n",
        "    # https://github.com/rxn4chemistry/rxn_yields/blob/master/nbs/06_data_augmentation.ipynb\n",
        "    Inspired from: https://github.com/undeadpixel/reinvent-randomized and https://github.com/GLambard/SMILES-X\n",
        "    Returns a random SMILES given a SMILES of a molecule.\n",
        "    :param mol: A Mol object\n",
        "    :param random_type: The type (unrestricted, restricted, rotated) of randomization performed.\n",
        "    :return : A random SMILES string of the same molecule or None if the molecule is invalid.\n",
        "    \"\"\"\n",
        "    mol = Chem.MolFromSmiles(smiles)\n",
        "    if not mol:\n",
        "        print(f\"{smiles} not valid.\")\n",
        "        return None\n",
        "\n",
        "    if random_type == \"unrestricted\":\n",
        "        return Chem.MolToSmiles(mol, canonical=False, doRandom=True, isomericSmiles=True)\n",
        "    elif random_type == \"restricted\":\n",
        "        new_atom_order = list(range(mol.GetNumAtoms()))\n",
        "        random.shuffle(new_atom_order)\n",
        "        random_mol = Chem.RenumberAtoms(mol, newOrder=new_atom_order)\n",
        "        return Chem.MolToSmiles(random_mol, canonical=False, isomericSmiles=True)\n",
        "    elif random_type == 'rotated':\n",
        "        n_atoms = mol.GetNumAtoms()\n",
        "        rotation_index = random.randint(0, n_atoms-1)\n",
        "        atoms = list(range(n_atoms))\n",
        "        new_atoms_order = (atoms[rotation_index%len(atoms):]+atoms[:rotation_index%len(atoms)])\n",
        "        rotated_mol = Chem.RenumberAtoms(mol,new_atoms_order)\n",
        "        return Chem.MolToSmiles(rotated_mol, canonical=False, isomericSmiles=True)\n",
        "    raise ValueError(\"Type '{}' is not valid\".format(random_type))"
      ],
      "metadata": {
        "id": "MGrUMhzzMRTR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_smi = 'O=C1C2=C(N=CN2C)N(C(=O)N1C)C'\n",
        "mol = Chem.MolFromSmiles(example_smi)\n",
        "print(f\"The canonical SMILES of this caffeine molecule is: {Chem.MolToSmiles(mol)}\")\n",
        "mol"
      ],
      "metadata": {
        "id": "TI-zZVWgNcHc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# different starting atom\n",
        "rotated_random_smiles = []\n",
        "for i in range (500):\n",
        "    rotated_random_smiles.append(randomize_smiles(example_smi))\n",
        "print(len(set(rotated_random_smiles)))\n",
        "set(rotated_random_smiles)"
      ],
      "metadata": {
        "id": "FnX839OUNcr7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "restricted_random_smiles = []\n",
        "for i in range (500):\n",
        "    restricted_random_smiles.append(randomize_smiles(example_smi, 'restricted'))\n",
        "print(len(set(restricted_random_smiles)))\n",
        "list(set(restricted_random_smiles))[:5]"
      ],
      "metadata": {
        "id": "MM61fdm5PKX5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unrestricted_random_smiles = []\n",
        "for i in range (10000):\n",
        "    unrestricted_random_smiles.append(randomize_smiles(example_smi, random_type='unrestricted'))\n",
        "print(len(set(unrestricted_random_smiles)))\n",
        "list(set(unrestricted_random_smiles))[:5]"
      ],
      "metadata": {
        "id": "SSJiNnttPZLO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "recanonicalised_smiles = set([Chem.MolToSmiles(Chem.MolFromSmiles(smiles)) for smiles in unrestricted_random_smiles])\n",
        "assert len(recanonicalised_smiles) == 1\n",
        "recanonicalised_smiles"
      ],
      "metadata": {
        "id": "842reYtbPoQ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(5):\n",
        "  print(randomize_smiles(can_smiles))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xaC-1e4RzSVC",
        "outputId": "c749fe1f-e5ca-41fa-e500-37e9f13192e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "C1COCCO1.COc1cc2c3c(c1)c(=O)c(Cc1cccnc1)cn3-c1cc(Br)ccc1S2.N#CC1=C(C#N)C(=O)C(Cl)=C(Cl)C1=O\n",
            "c1ncc(Cc2cn3c4c(cc(OC)cc4c2=O)Sc2c-3cc(Br)cc2)cc1.N#CC1=C(C#N)C(=O)C(Cl)=C(Cl)C1=O.C1COCCO1\n",
            "C1(C#N)=C(C#N)C(=O)C(Cl)=C(Cl)C1=O.C1COCCO1.COc1cc2c3c(c1)c(=O)c(Cc1cccnc1)cn3-c1cc(Br)ccc1S2\n",
            "c1cncc(Cc2cn3c4c(cc(OC)cc4c2=O)Sc2c-3cc(Br)cc2)c1.N#CC1=C(C#N)C(=O)C(Cl)=C(Cl)C1=O.C1COCCO1\n",
            "n12c3c(cc(OC)cc3c(=O)c(Cc3cccnc3)c1)Sc1c-2cc(Br)cc1.N#CC1=C(C#N)C(=O)C(Cl)=C(Cl)C1=O.C1COCCO1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# we will include a rotated copy of all the training reactions\n",
        "\n",
        "rotated_train_precursors = [randomize_smiles(precursors) for precursors in tqdm(train_df.precursors)]"
      ],
      "metadata": {
        "id": "p2jfdr_UQDkH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rotated_train_df = pd.DataFrame({'precursors': rotated_train_precursors, 'products': products_train})\n",
        "total_train_df = pd.concat([train_df, rotated_train_df])\n",
        "total_train_df.shape\n"
      ],
      "metadata": {
        "id": "8pXEayqQqyJt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To be able to train a language model, we need to split the strings into tokens\n",
        "\n",
        "# We take the regex pattern introduced in the [Molecular Transformer](https://pubs.acs.org/doi/abs/10.1021/acscentsci.9b00576).\n",
        "SMI_REGEX_PATTERN =  r\"(\\%\\([0-9]{3}\\)|\\[[^\\]]+]|Br?|Cl?|N|O|S|P|F|I|b|c|n|o|s|p|\\||\\(|\\)|\\.|=|#|-|\\+|\\\\|\\/|:|~|@|\\?|>>?|\\*|\\$|\\%[0-9]{2}|[0-9])\"\n",
        "\n",
        "def smiles_tokenizer(smiles):\n",
        "  smiles_regex = re.compile(SMI_REGEX_PATTERN)\n",
        "  tokens = [token for token in smiles_regex.findall(smiles)]\n",
        "  return ' '.join(tokens)"
      ],
      "metadata": {
        "id": "osc8wbDTsvyW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "IFuhH4TsxjRE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# remember to shuffle your training data :)\n",
        "\n",
        "shuffled_total_train_df = total_train_df.sample(frac=1., random_state=42)"
      ],
      "metadata": {
        "id": "xsTFqoLZsTSF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "EaTbQWErstvf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}